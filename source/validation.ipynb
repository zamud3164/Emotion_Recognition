{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu selected\n"
     ]
    }
   ],
   "source": [
    "# set device to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'{device} selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_dict = {'ang': 0,\n",
    "                'hap': 1,\n",
    "                'sad': 2,\n",
    "                'fea': 3,\n",
    "                'sur': 4,\n",
    "                'neu': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_text = pd.read_csv('../data/text_test.csv')\n",
    "y_test_text = x_test_text['label']\n",
    "\n",
    "x_test_audio = pd.read_csv('../data/audio_test_v0.csv')\n",
    "y_test_audio = x_test_audio['label']\n",
    "\n",
    "y_test = y_test_audio  # since y_train_audio == y_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1960, 422)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the 'transcription' columns directly\n",
    "transcription_test = x_test_text['transcription']\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "# Apply TfidfVectorizer\n",
    "x_test_text = tfidf.fit_transform(transcription_test).toarray()\n",
    "\n",
    "print(x_test_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1960, 698)\n"
     ]
    }
   ],
   "source": [
    "combined_x_test = np.concatenate((np.array(x_test_audio[x_test_audio.columns[2:]]), x_test_text), axis=1)\n",
    "\n",
    "print(combined_x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to make dummy input channel for CNN input feature tensor\n",
    "X_test = np.expand_dims(combined_x_test,1)\n",
    "\n",
    "# convert emotion labels from list back to numpy arrays for PyTorch to work with\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, dropout=0.2)\n",
    "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2, dropout=0.2)\n",
    "        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=1, dropout=0.2)\n",
    "        self.layer4 = self._make_layer(block, 128, num_blocks[3], stride=2, dropout=0.2)\n",
    "        self.linear = nn.Linear(128*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride, dropout):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        layers.append(nn.Dropout(dropout))  # Add dropout layer after the block\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool1d(out, out.size(2))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out_logits = self.linear(out)\n",
    "        out_softmax = F.softmax(out_logits, dim=1)\n",
    "        return out_logits, out_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function: CrossEntropyLoss()\n",
    "def criterion(predictions, targets):\n",
    "    return nn.CrossEntropyLoss()(input=predictions, target=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_validate_fnc(model,criterion):\n",
    "    def validate(X,Y):\n",
    "\n",
    "        # don't want to update any network parameters on validation passes: don't need gradient\n",
    "        # wrap in torch.no_grad to save memory and compute in validation phase:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # set model to validation phase i.e. turn off dropout and batchnorm layers\n",
    "            model.eval()\n",
    "\n",
    "            # get the model's predictions on the validation set\n",
    "            output_logits, output_softmax = model(X)\n",
    "            predictions = torch.argmax(output_softmax,dim=1)\n",
    "\n",
    "            # calculate the mean accuracy over the entire validation set\n",
    "            accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "\n",
    "            # compute error from logits (nn.crossentropy implements softmax)\n",
    "            loss = criterion(output_logits,Y)\n",
    "\n",
    "        return loss.item(), accuracy*100, predictions\n",
    "    return validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = None\n",
    "\n",
    "def load_checkpoint(optimizer, model, filename):\n",
    "    checkpoint_dict = torch.load(filename)\n",
    "    epoch = checkpoint_dict['epoch']\n",
    "    model.load_state_dict(checkpoint_dict['model'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from ../data/model_checkpoints\\parallel_CNN1D_FINAL-004.pkl\n"
     ]
    }
   ],
   "source": [
    "# pick load folder\n",
    "load_folder = '../data/model_checkpoints'\n",
    "\n",
    "# pick the epoch to load\n",
    "epoch = '004'\n",
    "\n",
    "model_name = f'parallel_CNN1D_FINAL-{epoch}.pkl'\n",
    "\n",
    "# make full load path\n",
    "load_path = os.path.join(load_folder, model_name)\n",
    "\n",
    "## instantiate empty model and populate with params from binary\n",
    "#model = CNN1D(len(emotions_dict))\n",
    "model = ResNet1D(BasicBlock, [2, 2, 2, 2], num_classes=len(emotions_dict))\n",
    "#optimizer = torch.optim.SGD(model.parameters(),lr=0.001, weight_decay=1e-4, momentum=0.4)\n",
    "load_checkpoint(optimizer, model, load_path)\n",
    "\n",
    "print(f'Loaded model from {load_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 32.35%\n"
     ]
    }
   ],
   "source": [
    "# Move the model to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# reinitialize validation function with model from chosen checkpoint\n",
    "validate = make_validate_fnc(model,criterion)\n",
    "\n",
    "# Convert to tensors\n",
    "X_test_tensor = torch.tensor(X_test,device=device).float()\n",
    "y_test_tensor = torch.tensor(y_test,dtype=torch.long,device=device)\n",
    "\n",
    "test_loss, test_acc, predicted_emotions = validate(X_test_tensor,y_test_tensor)\n",
    "\n",
    "print(f'Test accuracy is {test_acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
